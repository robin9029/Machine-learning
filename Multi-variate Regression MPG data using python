'''
Multi-variate Regression for MPG data 

Pridict the Miles per galon by applying  Gradient descent Function
Multi-variate are [weight, horsepower] 
''' 

import requests, io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import  warnings
warnings.filterwarnings('ignore')
%matplotlib inline

#input data from URL
URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'

urlData = requests.get(URL).content                                                 #get the content text from URL
df = pd.read_csv(io.StringIO(urlData.decode('utf-8')), sep="\s+",header=None)       #convert TEXT with spaces to CSV column
print("Extracted Inputfrom URL: ")
print(df.head())

#2. Identify target variable and independent variable.
'''
Target Variable :MPG
Independent Variable : 'cylinders','displacement','horsepower','weight','acceleration',
            'model year', 'origin', 'car name'

'''
#3. Prepare the data file 
df.columns = ['mpg','cylinders','displacement','horsepower','weight','acceleration',
              'model year', 'origin', 'car name']

print("Adding Column name : ")
print(df.head())
df = df.drop('car name', axis=1)   #as of now car name is require for linear regression


# column "origin" is where car comes from  #ordinal categorical variable 
df['origin'] = df['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})
df = pd.get_dummies(df, columns=['origin'])     #dummy binary variables



print("Adding dummy for Origin  : ")
print(df.head())

#missing values for horsepower

df.horsepower.unique()
df = df[df.horsepower != '?']
df.horsepower = df.horsepower.astype('float')
print ("data Cleaning : ")
print("data after removing   ? : ")
print(df.head())


#normalization
df = (df - df.mean())/df.std()
df.head()

X = df.iloc[ :,[3,4]]                                  # weight & horsepower
ones = np.ones([X.shape[0],1])                         # ones matrix 1st column
X = np.concatenate((ones,X),axis=1)                    # X- Matrix column [ones,weight, horsepower] 

y = df.iloc[:,0:1].values                              #converts it from pandas.core.frame.DataFrame to numpy.ndarray  [MPG]
theta = np.zeros([1,3])                                #matrix [1 1 1 ]

#set hyper parameters
alpha = 0.0001                                         #learning_rate = alpha = 0.0001 
iters = 1000                                           #iteration =1000


#computecost                                           #compute cost - error
def computeCost(X,y,theta):
    tobesummed = np.power(((X @ theta.T)-y),2)
    return np.sum(tobesummed)/(2 * len(X))

#gradient descent Function 
# theta = theta -Learningrate/len_of(X)*sum (X * (X dot product theta transpose - y))

def gradientDescent(X,y,theta,iters,alpha):
    cost = np.zeros(iters)                                      # cost 1000                             
    #print(cost.shape)
    for i in range(iters):
        theta = theta - (alpha/len(X)) * np.sum(X * (X @ theta.T - y), axis=0)     # re assigning value to theta with new value 
        cost[i] = computeCost(X, y, theta)                      #compute cost 
    
    return theta,cost

#running the gd and cost function
g,cost = gradientDescent(X,y,theta,iters,alpha)                 #gradiant,cost function 
print("Gradient descent" ,g)

finalCost = computeCost(X,y,g)
print("Gradient descent Final cost",finalCost)

#plot the cost
fig, ax = plt.subplots()  
ax.plot(np.arange(iters), cost, 'r')  
ax.set_xlabel('Iterations')  
ax.set_ylabel('Cost')  
ax.set_title('Error vs. Training Epoch') 
